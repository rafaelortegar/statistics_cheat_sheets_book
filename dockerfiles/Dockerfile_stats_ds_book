# Use Python 3.10 as the base image
FROM python:3.10

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    bash \
    build-essential \
    curl \
    wget \
    git \
    openjdk-17-jre-headless \
    libffi-dev \
    libssl-dev \
    libbz2-dev \
    libreadline-dev \
    libsqlite3-dev \
    libpq-dev \
    zlib1g-dev && \
    rm -rf /var/lib/apt/lists/*

# Configure environment variables for Java and PySpark
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH
ENV SPARK_VERSION=3.4.1
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH

# Install Pyenv and Pyenv-Virtualenv
ENV PYENV_ROOT=/root/.pyenv
ENV PATH=$PYENV_ROOT/bin:$PYENV_ROOT/shims:$PATH

RUN rm -rf $PYENV_ROOT ~/.pyenv && \
    curl https://pyenv.run | bash && \
    if [ -d $PYENV_ROOT/plugins/pyenv-virtualenv ]; then rm -rf $PYENV_ROOT/plugins/pyenv-virtualenv; fi && \
    git clone https://github.com/pyenv/pyenv-virtualenv.git $PYENV_ROOT/plugins/pyenv-virtualenv && \
    echo 'export PYENV_ROOT="$HOME/.pyenv"' >> ~/.bashrc && \
    echo 'export PATH="$PYENV_ROOT/bin:$PATH"' >> ~/.bashrc && \
    echo 'eval "$(pyenv init --path)"' >> ~/.bashrc && \
    echo 'eval "$(pyenv init -)"' >> ~/.bashrc && \
    echo 'eval "$(pyenv virtualenv-init -)"' >> ~/.bashrc && \
    bash -c "source ~/.bashrc && pyenv install 3.10.12 && pyenv virtualenv 3.10.12 stats_ds_book_env && pyenv global stats_ds_book_env"

# Install Poetry
RUN curl -sSL https://install.python-poetry.org | python3 -
ENV PATH="/root/.local/bin:$PATH"

# Set working directory to project root
WORKDIR /app

# Copy the entire project into the container
COPY . /app

# Ensure /notebooks directory exists
RUN mkdir -p /app/book/notebooks

# Create a symbolic link for pyproject.toml in /notebooks
RUN ln -s /app/pyproject.toml /app/book/notebooks/pyproject.toml

# Install Python dependencies with Poetry
RUN bash -c "source ~/.bashrc && pyenv activate stats_ds_book_env && poetry env use $(pyenv which python) && poetry install"

# Install additional tools in the Pyenv environment
RUN bash -c "source ~/.bashrc && pyenv activate stats_ds_book_env && \
    pip install --no-cache-dir \
    pyspark \
    jupyter \
    jupyter-book \
    jupytext \
    flake8 \
    black \
    pre-commit"

# Install Jupyter
# RUN pip install jupyterlab
RUN bash -c "source ~/.bashrc && pyenv activate stats_ds_book_env && pip install jupyterlab"

# Configure Pre-Commit
RUN bash -c "source ~/.bashrc && pyenv activate stats_ds_book_env && pre-commit install"

# Download and install Apache Spark
RUN wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /opt && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Expose ports for Jupyter Notebook
EXPOSE 8888
EXPOSE 8000

# Default command to activate pyenv and launch Jupyter Lab
CMD ["bash", "-c", "source ~/.bashrc && pyenv activate stats_ds_book_env && cd /app && jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root && jupyter-book build . && jupyter-book serve --port 8000 --no-browser --watch ."]
